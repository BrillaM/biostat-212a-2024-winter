---
title: "Biostat 212a Homework 1"
subtitle: "Due Jan 23, 2024 @ 11:59PM"
author: "Brilla Meng UID: 806329681"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## Filling gaps in lecture notes (10pts)

Consider the regression model
$$
Y = f(X) + \epsilon,
$$
where $\operatorname{E}(\epsilon) = 0$. 

### Optimal regression function

Show that the choice
$$
f_{\text{opt}}(X) = \operatorname{E}(Y | X)
$$
minimizes the mean squared prediction error
$$
\operatorname{E}\{[Y - f(X)]^2\},
$$
where the expectations averages over variations in both $X$ and $Y$. (Hint: condition on $X$.)

**answer**:
$$
\operatorname{E}\{[Y - f(X)]^2|X=x\} \\
=\operatorname{E}[Y^2|X=x] - 2f(X)\operatorname{E}[Y|X=x]+f(x)^2 \\
$$
taking derivative with respect to f(x) 
$$
-2\operatorname{E}[Y|X=x]+2f(x)=0 \\
2f(x)=2\operatorname{E}[Y|X=x] \\
f(x)=\operatorname{E}[Y|X=x]
$$
Therefore, we found E[Y|X=x] is the minimizes the MSE.


### Bias-variance trade-off

Given an estimate $\hat f$ of $f$, show that the test error at a $x_0$ can be decomposed as
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = \underbrace{\operatorname{Var}(\hat f(x_0)) + [\operatorname{Bias}(\hat f(x_0))]^2}_{\text{MSE of } \hat f(x_0) \text{ for estimating } f(x_0)} + \underbrace{\operatorname{Var}(\epsilon)}_{\text{irreducible}},
$$
where the expectation averages over the variability in $y_0$ and $\hat f$.

**answer**:
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} \\
=\operatorname{E}\{[y_0 - f(x_0) + f(x_0) - \hat f(x_0)]^2\} \\
=\operatorname{E}\{[y_0 - f(x_0)]^2\} + \operatorname{E}\{[f(x_0) - \hat f(x_0)]^2\} + 2\operatorname{E}\{[y_0 - f(x_0)][f(x_0) - \hat f(x_0)]\} \\
$$
Since  E[$\epsilon$] = 0 and Y0 = f(x0) + $\epsilon$ ,we have
$$
\operatorname{E}\{[y_0 - f(x_0)]^2\} \\
=\operatorname{E}\{[f(x_0) + \epsilon - f(x_0)]^2\} \\
=\operatorname{E}\{[\epsilon]^2\} - (\operatorname{E}[\epsilon])^2 +(E[\epsilon])^2 \\
=\operatorname{Var}(\epsilon) \\
$$
Since Y0-f(x0) = $\epsilon$, E[$\epsilon$] = 0
$$
2(f(x_0) - \hat f(x_0))\operatorname{E}\{[y_0 - f(x_0)]\} \\
=2(f(x_0) - \hat f(x_0))\operatorname{E}\{[\epsilon]\} \\
=0\\
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} \\
=\operatorname{E}[(f(x_0) - \hat f(x_0))^2] + \operatorname{E}(\hat f(x_0))-\hat f(x_0)^2 \\
=\operatorname{E}[f(x_0)-E[\hat f(x_0)]]^2 + E[\hat f(x_0)]-\hat f(x_0)]]^2 \\
=Bias^2[\hat f(x_0)] + Var[\hat f(x_0)] 
$$
Combine all of them, we can got:
$$
\operatorname{E}\{[y_0 - \hat f(x_0)]^2\} = Var(\hat f(x_0)) + Bias^2(\hat f(x_0)) + Var(\epsilon)
$$

## ISL Exercise 2.4.3 (10pts)
a.
```{r, eval = F}
library(tidyverse)
flexibility <- seq(1, 10, length.out = 100)
bias_squared <- (10 - flexibility)^2 / 100
variance <- flexibility^2 / 100
training_error <- bias_squared - (flexibility / 50) + 0.2
irreducible_error <- rep(0.2, length(flexibility))
test_error <- bias_squared + variance + irreducible_error
data <- data.frame(flexibility, bias_squared, variance, 
                   training_error, test_error, irreducible_error)
library(reshape2)
data_melted <- melt(data, id.vars = 'flexibility')
ggplot(data_melted, aes(x = flexibility, y = value, color = variable)) +
  geom_line() +
  labs(x = 'Flexibility', y = 'Error', title = 'Bias-Variance Decomposition') +
  theme_minimal() +
  scale_color_discrete(name = "Curves", labels = c("Squared Bias", "Variance", "Training Error", 
                                                   "Test Error", "Bayes (Irreducible) Error"))
```
b.
Test error decreases initially but rises as flexibility leads to overfitting.Test error decreases initially but rises as flexibility leads to overfitting.   Bias decreases, improving problem representation, while variance increases, especially at higher flexibility, reducing model robustness.   This highlights the challenge in balancing model complexity for optimal performance.


## ISL Exercise 2.4.4 (10pts)
a.
1.
Medical Diagnosis:
Response: Diagnosis (e.g., disease present or not).
Predictors: Patient symptoms, lab test results, demographic data (age, gender), medical history.
Goal: The goal is primarily prediction. The emphasis is on accurately predicting whether a patient has a specific disease or condition based on their symptoms and test results. While inference can be valuable for understanding which factors are most predictive of certain diseases, the immediate utility is in the accurate and efficient prediction of the disease for treatment decisions.
2.
Credit Scoring in Finance:
Response: Creditworthiness (e.g., high or low credit risk).
Predictors: Credit history, current debts, income, employment status, past loan repayment history, credit score.
Goal: This application leans towards prediction. Financial institutions use these models to predict the likelihood that an individual will repay a loan. Understanding the factors that influence credit risk is important, but the primary objective is to predict an individual's credit risk to make lending decisions.
3.
Customer Churn Prediction in Business:
Response: Churn (e.g., whether a customer will stop using a company's products/services).
Predictors: Customer interaction history, purchase history, customer service records, demographic data, usage patterns.
Goal: Again, the goal is prediction. Companies use these models to predict which customers are at risk of leaving so that they can take proactive measures to retain them. While inference might help understand why customers churn, the direct aim is to predict churn to implement retention strategies.

b.
1.
Real Estate Pricing:
Response: House price.
Predictors: Size (square footage), location, number of bedrooms, age of the house, proximity to amenities, etc.
Goal: This application is primarily for prediction. The focus is on predicting the price of a house based on various features, which is crucial for buyers, sellers, and real estate agents.
2.
Weather Forecasting:
Response: Temperature.
Predictors: Humidity, atmospheric pressure, wind speed, historical temperature data, time of the year, etc.
Goal: The goal is prediction. Accurate temperature forecasts based on current and historical weather data are vital for a range of activities, from agriculture to daily planning.
3.
Educational Outcomes:
Response: Student academic performance (e.g., grades or test scores).
Predictors: Study hours, attendance, parental education level, socioeconomic status, previous academic records, etc.
Goal: This can be for both inference and prediction. While predicting student performance is valuable, understanding the impact of various factors (like study hours or socioeconomic status) on academic outcomes is also crucial for educational policy and interventions.

c.
1.
Market Segmentation:
Objective: To categorize customers into different segments based on their purchasing behavior, preferences, demographic characteristics, etc.
Application: Companies can use cluster analysis to identify distinct groups within their customer base. This helps in tailoring marketing strategies, developing targeted products, and improving customer service by understanding the specific needs and preferences of each segment.
2.
Genomic Data Classification in Biology:
Objective: To classify genetic data for identifying patterns and similarities in DNA sequences.
Application: In biological research, cluster analysis is employed to group genes with similar expression patterns, which can be indicative of shared functions or regulatory mechanisms. This is crucial in understanding genetic diseases, evolutionary biology, and the development of targeted treatments.
3.
Document Classification:
Objective: To organize and categorize large sets of digital documents based on their content and thematic similarities.
Application: This is particularly useful in digital libraries, online research databases, and for information retrieval systems. By clustering documents, these systems can enhance search accuracy, improve the organization of information, and enable users to discover related content more effectively.

## ISL Exercise 2.4.10 (30pts)

Your can read in the `boston` data set directly from url <https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv>. A documentation of the `boston` data set is [here](https://www.rdocumentation.org/packages/ISLR2/versions/1.3-2/topics/Boston).


a.
#### R

```{r, evalue = F}
library(tidyverse)

Boston <- read_csv("https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/Boston.csv", 
                   col_select = -1) %>% 
  print(width = Inf)
```
**answer**:
There is 506 rows and 13 columns in the data set. Each row represent the set of predictor obeservations for a given Neighborhood in Boston. Each column represent each predictor variable for which an observation was made in 506 neighborhoods of Boston.

b.
```{r, evalue = F}
str(Boston)
Boston$chas <- as.numeric(Boston$chas)
Boston$rad <- as.numeric(Boston$rad)
pairs(Boston)
```

**answer**:
Not much can be discerned other than the fact that some variables appear to be correlated.

c.
```{r, evalue = F}
cor(Boston)
```
**answer**:
The variables that are most correlated with `medv` are `lstat` and `rm`. The variables that are most correlated with `lstat` are `rm` and `ptratio`. The variables that are most correlated with `rm` are `lstat` and `ptratio`.

d.
```{r, evalue = F}
summary(Boston$crim)
summary(Boston$tax)
summary(Boston$ptratio)
qplot(Boston$crim, binwidth=5 , xlab = "Crime rate", ylab="Number of Suburbs" )
qplot(Boston$tax, binwidth=50 , xlab = "Full-value property-tax rate per $10,000", ylab="Number of Suburbs")
qplot(Boston$ptratio, binwidth=5, xlab ="Pupil-teacher ratio by town", ylab="Number of Suburbs")
selection <- subset( Boston, crim > 10)
nrow(selection)/ nrow(Boston)
selection <- subset( Boston, crim > 60)
nrow(selection)/ nrow(Boston)
selection <- subset( Boston, tax > 600)
nrow(selection)/ nrow(Boston)
selection <- subset( Boston, tax < 600)
nrow(selection)/ nrow(Boston)
selection <- subset( Boston, ptratio > 17.5)
nrow(selection)/ nrow(Boston)
selection <- subset( Boston, ptratio < 17.5)
nrow(selection)/ nrow(Boston)
```
**answer**:
For the crime rate, the median is 0.25%, and the maximum is 88.976%, there are some neighborhoods with very high crime rates. And 11% of the neighborhoods have crime rates above 10%, and 0.6% of the neighborhoods have crime rates above 60%.
Based on the histogram, there are few place where the tax rate are very high. The median is 330, and the mean is 408.2,the maximum is 711. 27% of the neighborhoods have tax rates above 600. 73% of the neighborhoods have tax rates below 600.
Based on the histogram of pupil-teacher ratio, the median is 19.05, and the mean is 18.5, the maximum is 22. . 72% of the neighborhoods have pupil-teacher ratio above 17.5, and 28% of the neighborhoods have pupil-teacher ratio below 17.5.

e.
```{r, evalue = F}
nrow(subset(Boston, chas ==1)) 
```
**answer**:
There are 35 census tracts that bound the Charles River.

f.
```{r, evalue = F}
summary(Boston$ptratio)
```
**answer**:
The median pupil-teacher ratio among the towns in this data set is 19.05.

g.
```{r, evalue = F}
selection <- Boston[order(Boston$medv),]
selection[1,]
summary(selection)
```
**answer**: The lowest median value of owner- occupied homes is 5. 
This census tract has several distinctive characteristics compare to overall range. The crime rate is very high at 38.3518, far exceeding the median range, this area has an usually high crime rate.The tract has no residential land zoned for large lots , is entirely industrial , and has an older housing stock , indicating it is a fully developed, non-residential area. The nox is very high because of the industrial pollution.The average number of rooms  is lower at 5.453, and the distance to employment centers is quite close at 1.4896. The accessibility to radial highways is the highest at 24, and the property-tax rate is very high at 666. The pupil-teacher ratio is also on the higher end at 20.2, and the proportion of African Americans  is near the maximum at 396.9. Lastly, the lower status of the population  is very high at 30.59, and the median value of owner-occupied homes  is very low at 5. These factors collectively suggest an urban, high-crime, industrial area with economic challenges.

h.
```{r, evalue = F}
rm_over_7 <- subset(Boston, rm>7)
nrow(rm_over_7) 

rm_over_8 <- subset(Boston, rm>8)
nrow(rm_over_8) 
summary(rm_over_8)
```
**answer**: 
There are 64 neighborhoods with more than 7 rooms, and 13 neighborhoods with more than 8 rooms. The median value of homes in neighborhoods with more than 8 rooms is 45,000 dollars higher than the median value of homes in neighborhoods with more than 7 rooms. The mean value of homes in neighborhoods with more than 8 rooms is 50,000 dollars higher than the mean value of homes in neighborhoods with more than 7 rooms.

## ISL Exercise 3.7.3 (12pts)

a.
Salary=50+20*(GPA)+0.07⋅(IQ)+35⋅(Level)+0.01⋅(GPA×IQ)−10⋅(GPA×Level)

i.Salary = 50 + 20 * GPA + 0.07 * IQ + 35 * College + 0.01 * GPA * IQ - 10 * GPA * College 
We now can estimate that High school earn an average of 50 + 20 * mean(GPA) + 0.07 * mean(IQ) + 0.01 * mean(GPA) * mean(IQ) and College earn an average of 50 + 20 * mean(GPA) + 0.07 * mean(IQ) + 35 + 0.01 * mean(GPA) * mean(IQ) - 10 * mean(GPA). When you subtract out the common terms, you find out that College earn an average of 35 - 10 * mean(GPA) more than High School. Since we don't know the value of mean(GPA), we don't know whether High school are outearning College on average or not.

ii.That's also uncertain.

iii.Since College earn an average of 35 - 10 * mean(GPA) more than High School, a higher GPA means College earn less than High school. So this is true statement. 

iv.Since College earn an average of 35 - 10 * mean(GPA) more than High School, a higher GPA means College earn less than High school. So this is false statement.

**answer**: The answer is iii.

b.
We estimate that College earn an average of 50+20⋅(GPA)+0.07⋅(IQ)+35⋅(1)+0.01⋅(GPA×IQ)−10⋅(GPA×1).
When we add IQ as 110 and GPA as 4.0.we will get 50+20⋅4.0+0.07⋅110+35⋅(1)+0.01⋅110×4.0−10⋅(4.0×1)
= 137.1
**answer**: The estimate salary of a college graduate with IQ of 110 and a GPA of 4.0 is $137100.

c.
**answer**: 
False, the magnitude of the coefficient is not an indicator of statistical significance.



## ISL Exercise 3.7.15 (20pts)

a.
Model:Y(crim) = β0 + β1 (zn)X
```{r, evalue = F}
data = Boston
boston.zn <- lm(crim ~ zn, data=Boston)
 summary(boston.zn)
 par(mfrow = c(2, 2))
 plot(boston.zn)
```
we can see that F-statistic is 21.1 and p-value is < 5.506e-06,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and zn.

Model:Y(crim) = β0 + β1 (indus)X
```{r, evalue = F}
boston.indus <- lm(crim ~ indus, data=Boston)
 summary(boston.indus)
 par(mfrow = c(2, 2))
 plot(boston.indus)
```
we can see that F-statistic is 99.82 and p-value is < 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and indus.

Model:Y(crim) = β0 + β1 (chas)X
```{r, evalue = F}
boston.chas <- lm(crim ~ chas, data=Boston)
 summary(boston.chas)
 par(mfrow = c(2, 2))
 plot(boston.chas)
```
we can see that F-statistic is 1.579 and p-value is 0.2094. There is  not a statistically significant association between crim and chas.

Model:Y(crim) = β0 + β1 (nox)X
```{r, evalue = F}
boston.nox <- lm(crim ~ nox, data=Boston)
 summary(boston.nox)
 par(mfrow = c(2, 2))
 plot(boston.nox)
```
we can see that F-statistic is 108.6 and p-value is < 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and nox.

Model:Y(crim) = β0 + β1 (rm)X
```{r, evalue = F}
boston.rm <- lm(crim ~ rm, data=Boston)
 summary(boston.rm)
 par(mfrow = c(2, 2))
 plot(boston.rm)
```
we can see that F-statistic is 25.45 and p-value is 6.347e-07,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and rm.

Model:Y(crim) = β0 + β1 (age)X
```{r, evalue = F}
boston.age <- lm(crim ~ age, data=Boston)
 summary(boston.age)
 par(mfrow = c(2, 2))
 plot(boston.age)
```
we can see that F-statistic is 71.62 and p-value is 2.855e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and age.

Model:Y(crim) = β0 + β1 (dis)X
```{r, evalue = F}
boston.dis <- lm(crim ~ dis, data=Boston)
 summary(boston.dis)
 par(mfrow = c(2, 2))
 plot(boston.dis)
```
we can see that F-statistic is 84.89 and p-value is 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and dis.

Model:Y(crim) = β0 + β1 (rad)X
```{r, evalue = F}
boston.rad <- lm(crim ~ rad, data=Boston)
 summary(boston.rad)
 par(mfrow = c(2, 2))
 plot(boston.rad)
```
we can see that F-statistic is 323.9 and p-value < 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and red.

Model:Y(crim) = β0 + β1 (tax)X
```{r, evalue = F}
boston.tax <- lm(crim ~ tax, data=Boston)
 summary(boston.tax)
 par(mfrow = c(2, 2))
 plot(boston.tax)
```
we can see that F-statistic is 259.2 and p-value < 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and tax.

Model:Y(crim) = β0 + β1 (ptratio)X
```{r, evalue = F}
boston.ptratio <- lm(crim ~ ptratio, data=Boston)
 summary(boston.ptratio)
 par(mfrow = c(2, 2))
 plot(boston.ptratio)
```
we can see that F-statistic is 46.26 and p-value is 2.943e-11,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and ptratio.

Model:Y(crim) = β0 + β1 (lstat)X
```{r, evalue = F}
boston.lstat <- lm(crim ~ lstat, data=Boston)
 summary(boston.lstat)
 par(mfrow = c(2, 2))
 plot(boston.lstat)
```
we can see that F-statistic is 132 and p-value < 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and lstat.

Model:Y(crim) = β0 + β1 (medv)X
```{r, evalue = F}
boston.medv <- lm(crim ~ medv, data=Boston)
 summary(boston.medv)
 par(mfrow = c(2, 2))
 plot(boston.medv)
```
we can see that F-statistic is 89.49 and p-value < 2.2e-16,meaning the chance of having a null hypothesis (β0) is very low. There is a statistically significant association between crim and medv.

b.
```{r, evalue = F}
lm.all <- lm(crim ~.,data = Boston)
summary(lm.all)
```
**answer**: The predictors that appear to be statistically significant are zn, dis, rad, medv  have a significant association with crim (p-value is below 0.05) which means we can reject the null hypothesis.

c.
```{r, evalue = F}
x = c(coefficients(boston.zn)[2],
      coefficients(boston.indus)[2],
      coefficients(boston.chas)[2],
      coefficients(boston.nox)[2],
      coefficients(boston.rm)[2],
      coefficients(boston.age)[2],
      coefficients(boston.dis)[2],
      coefficients(boston.rad)[2],
      coefficients(boston.tax)[2],
      coefficients(boston.ptratio)[2],
      coefficients(boston.lstat)[2],
      coefficients(boston.medv)[2])
y = coefficients(lm.all)[2:13]
plot(x, y, col = "blue",pch =19, ylab = "multiple regression coefficients",
     xlab = "Univariate Regression coefficients",
     main = "Relationship between Multiple regression \n and univariate regression coefficients")
```
d.
Model: crim=β0+β1(zn)+β2(zn)2+β3(zn)3+ϵ
```{r, evalue = F}
boston.poly.zn = lm(crim ~ zn + I(zn^2) + I(zn^3), data = Boston)
summary(boston.poly.zn)
```
Model: crim=β0+β1(indus)+β2(indus)2+β3(indus)3+ϵ
```{r, evalue = F}
boston.poly.indus = lm(crim ~ indus + I(indus^2) + I(indus^3), data = Boston)
summary(boston.poly.indus)
```

Model: crim=β0+β1(chas)+β2(chas)2+β3(chas)3+ϵ
```{r, evalue = F}
boston.poly.chas = lm(crim ~  + I(chas^2) + I(chas^3), data = Boston)
summary(boston.poly.chas)
```

Model: crim=β0+β1(nox)+β2(nox)2+β3(nox)3+ϵ
```{r, evalue = F}
boston.poly.nox = lm(crim ~ nox + I(nox^2) + I(nox^3), data = Boston)
summary(boston.poly.nox)
```

Model: crim=β0+β1(rm)+β2(rm)2+β3(rm)3+ϵ
```{r, evalue = F}
boston.poly.rm = lm(crim ~ rm + I(rm^2) + I(rm^3), data = Boston)
summary(boston.poly.rm)
```

Model: crim=β0+β1(age)+β2(age)2+β3(age)3+ϵ
```{r, evalue = F}
boston.poly.age = lm(crim ~ age + I(age^2) + I(age^3), data = Boston)
summary(boston.poly.age)
```

Model: crim=β0+β1(dis)+β2(dis)2+β3(dis)3+ϵ
```{r, evalue = F}
boston.poly.dis = lm(crim ~ dis + I(dis^2) + I(dis^3), data = Boston)
summary(boston.poly.dis)
```

Model: crim=β0+β1(rad)+β2(rad)2+β3(rad)3+ϵ
```{r, evalue = F}
boston.poly.rad = lm(crim ~ rad + I(rad^2) + I(rad^3), data = Boston)
summary(boston.poly.rad)
```

Model: crim=β0+β1(tax)+β2(tax)2+β3(tax)3+ϵ
```{r, evalue = F}
boston.poly.tax = lm(crim ~ tax + I(tax^2) + I(tax^3), data = Boston)
summary(boston.poly.tax)
```

Model: crim=β0+β1(ptratio)+β2(ptratio)2+β3(ptratio)3+ϵ
```{r, evalue = F}
boston.poly.ptratio = lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3), data = Boston)
summary(boston.poly.ptratio)
```

Model: crim=β0+β1(lstat)+β2(lstat)2+β3(lstat)3+ϵ
```{r, evalue = F}
boston.poly.lstat = lm(crim ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)
summary(boston.poly.lstat)
```

Model: crim=β0+β1(medv)+β2(medv)2+β3(medv)3+ϵ
```{r, evalue = F}
boston.poly.medv = lm(crim ~ medv + I(medv^2) + I(medv^3), data = Boston)
summary(boston.poly.medv)
```
**answer** :Each of these variables [indus, nox, dis, ptratio, medv] squared and cubed terms are statistically significant (p-value is below 0.05) which means we can reject the null hypothesis. Age seems like have non-linear relationship with crim, but the squared and cubed terms are not statistically significant (p-value is above 0.05) which means we can not reject the null hypothesis. For other variable, there are no evidence to support the non-linear relationship between crim and other variables.



## Bonus question (20pts)

For multiple linear regression, show that $R^2$ is equal to the correlation between the response vector $\mathbf{y} = (y_1, \ldots, y_n)^T$ and the fitted values $\hat{\mathbf{y}} = (\hat y_1, \ldots, \hat y_n)^T$. That is
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = [\operatorname{Cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
$$
**answer** :
Recall that 
$$
Cor(x,y) = \frac{\sum_{i}(x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i}(x_i - \overline{x})^2 \sum_{i}(y_i - \overline{y})^2}} 
= \frac{(x - \frac{1}{n}J_n)^T(y - \frac{1}{n}J_n)}
{\sqrt{(x - \frac{1}{n}J_n)^T(x - \frac{1}{n}J_n)(y - \frac{1}{n}J_n)^T(y - \frac{1}{n}J_n)}}
$$
where
$$
J = 11^T = 
\begin{bmatrix}
    1 & 1 & \dots & 1 \\
    1 & 1 & \dots & 1 \\
    \vdots & \vdots & \ddots & \vdots \\
    1 & 1 & \dots & 1
\end{bmatrix}_{n \times n}
$$
And for R^2
$$
R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i}(y_i - \hat{y}_i)^2}{\sum_{i}(y_i - \bar{y})^2}
$$
And
$$
R^2 = 1 - \frac{\sum_{i}(y_i - \hat{y}_i)^2}{\sum_{i}(y_i - \bar{y})^2} = 1 - \frac{(Y - HY)^T(Y - HY)}{(Y - \frac{1}{n}J_nY)^T(Y - \frac{1}{n}J_nY)} = 1 - \frac{Y^T(I - H)^T(I - H)Y}{Y^T(I - \frac{1}{n}J_n)^T(I - \frac{1}{n}J_n)Y}
$$
Therefore,
$$
R^2 = 1 - \frac{Y^T(I - H)Y}{Y^T(I - \frac{1}{n}J)Y}
$$
where H = X(X^TX)^{-1}X^T is a projection matrix and so as I - H.
Now we have
$$
Cor(Y, \hat{Y})^2 = \frac{(Y - \frac{1}{n}JY)^T(\hat{Y} - \frac{1}{n}J\hat{Y})}
{[(Y - \frac{1}{n}JY)^T(Y - \frac{1}{n}JY)][(\hat{Y} - \frac{1}{n}J\hat{Y})^T(\hat{Y} - \frac{1}{n}J\hat{Y})]}
$$
where
$$
\hat{Y} = X\beta = X(X^TX)^{-1}X^TY = HY
$$