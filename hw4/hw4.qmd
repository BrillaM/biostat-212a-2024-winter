---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 5, 2024 @ 11:59PM"
author: "Brilla Meng UID: 806329681"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 8.4.3 (10pts)
```{r}
library(ISLR)
library(tree)
library(rpart)
library(caret)
library(randomForest)
library(imager)
```

```{r}
p = seq(0, 1, by = 0.001)
qini.index = 2 * p *(1-p)
class.error = 1 - pmax(p, 1-p)
cross.entropy = - (p * log(p) + (1-p) * log(1-p))
matplot(p, cbind(qini.index, class.error, cross.entropy), 
        type = "l", 
        lty = 1, 
        col = 1:3,
        xlab = "p", 
        ylab = "Gini index, classification error,cross-entropy")
legend("bottom",
       legend=c("Gini Index", "Classification Error", "Cross-Entropy"), 
       col=c("blue", "red", "green"), lty=1)
```

## ISL Exercise 8.4.4 (10pts)

a.

```{r}
knitr::include_graphics("/Users/brilla/Desktop/MDSH/BIOSTAT 212A/biostat-212a-2024-winter/hw4/hw1.jpeg")
```

b.
```{r}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
# X2 < 1
lines(x = c(-2, 2), y = c(1, 1))
# X1 < 1 with X2 < 1
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))
# X2 < 2 with X2 >= 1
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))
```

## ISL Exercise 8.4.5 (10pts)

```{r}
probs <- c(0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7, 0.75)
sum(probs >= 0.5) #number of red
sum(probs < 0.5) #number of green
ifelse(sum(probs >= 0.5) > sum(probs < 0.5), "red", "green")
mean(probs) #avg of P(red)
ifelse(mean(probs) > 0.5, "red", "green")
```
## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, and boosting methods for predicting `medv`. Evaluate out-of-sample performance on a test set.

```{r} 
library(tidyverse)
library(GGally)
library(gtsummary)
library(ranger)
library(tidymodels)
library(ISLR2)
library(rpart.plot)
library(vip)
library(xgboost)     
```
```{r}
set.seed(1)
data_split <- initial_split(
  Boston, 
  prop = 0.5
  )
data_split
Boston_train <- training(data_split)
dim(Boston_train)
Boston_test <- testing(data_split)
dim(Boston_test)
```
```{r}
tree_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_train ) |>
  step_naomit(medv) |>
  step_zv(all_numeric_predictors())
tree_recipe
regtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "regression",
  engine = "rpart"
  ) 
regtree_mod 
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(regtree_mod)
```
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100, 5))
set.seed(1)
folds <- vfold_cv(Boston_train, v = 5)
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(rmse, rsq)
  )
```
```{r}
best_tree <- tree_fit %>%
  select_best("rmse")
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
```
```{r}
final_fit <- final_wf %>%
  last_fit(data_split)
final_fit %>%
  collect_metrics()
```
```{r}
final_tree <- extract_workflow(final_fit)
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
final_tree %>%
 extract_fit_parsnip() %>% 
  vip()
```
# random forest

```{r random_forest}
rf_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_train ) |>
  step_naomit(medv) |>
  step_zv(all_numeric_predictors())
rf_mod <- 
  rand_forest(
    mode = "regression",
    mtry = tune(),
    trees = tune()
  ) %>% 
  set_engine("ranger")
```
```{r}
rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
```
```{r}
set.seed(1)
folds <- vfold_cv(Boston_train, v = 5)
```
```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
```
```{r}
best_rf <- rf_fit %>%
  select_best("rmse")
rf_final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
rf_final_fit <- rf_final_wf %>%
  last_fit(data_split)
rf_final_fit %>%
  collect_metrics()
```
# Boosting

```{r}
gb_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_train ) |>
  step_naomit(medv) |>
  step_zv(all_numeric_predictors())
gb_recipe
gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 1000,
    tree_depth = tune(),
    learn_rate = tune()) %>%
  set_engine("xgboost"
  )
gb_mod
```
```{r}
gb_wf <- workflow() |>
  add_recipe(gb_recipe) |>
  add_model(gb_mod)
gb_param_grid <- grid_regular(
  tree_depth(range = c(1L, 4L)),
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(4, 10)
  )
```
```{r}
set.seed(1)
folds <- vfold_cv(Boston_train, v = 5)
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = gb_param_grid,
    metrics = metric_set(rmse, rsq)
  )
```
```{r}
best_gb <- gb_fit %>%
  select_best("rmse")
gb_final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
gb_final_fit <- gb_final_wf %>%
  last_fit(data_split)
gb_final_fit %>%
  collect_metrics()
gb_final_tree <- extract_workflow(gb_final_fit)
gb_final_tree %>%
  extract_fit_engine() %>%
  vip()
```

## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. Evaluate out-of-sample performance on a test set.


```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2) 
data(Carseats)

```
```{r}
High<- factor(ifelse(Carseats$Sales > 8, "yes", "no"))
Carseats_class <- data.frame(Carseats, High)
Carseats_class <- Carseats_class %>% select(-Sales)
```
```{r}
set.seed(123)
data_split <- initial_split(Carseats_class, prop = 0.5, strata = High)
data_split
Carseats_train <- training(data_split)
dim(Carseats_train)
Carseats_test <- testing(data_split)
dim(Carseats_test)
```
```{r}
class_tree_recipe <- 
  recipe(High ~ ., data = Carseats_train) |>
  step_naomit(all_predictors()) |>
  step_dummy(all_nominal(), -all_outcomes()) |>
  step_center(all_predictors(), -all_outcomes()) |>
  step_scale(all_predictors(), -all_outcomes())
class_tree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "classification",
  engine = "rpart"
  ) 
```
```{r}
class_tree_workflow <- 
  workflow() %>%
  add_recipe(class_tree_recipe) %>%
  add_model(class_tree_mod)
```
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100,5))
```
```{r}
set.seed(123)
class_folds <- vfold_cv(Carseats_train, v = 5)
class_tree_fit <- class_tree_workflow %>%
  tune_grid(
    resamples = class_folds,
    grid = tree_grid,
    metrics = metric_set(roc_auc)
  )
```
```{r}
class_best_tree <- class_tree_fit %>%
  select_best(metric = "roc_auc")
class_best_tree
class_final_workflow <- class_tree_workflow %>%
  finalize_workflow(class_best_tree)
class_final_fit <- class_final_workflow %>%
  last_fit(data_split)
class_final_fit %>%
  collect_metrics()
```
```{r}
class_final_tree <- extract_workflow(class_final_fit)
class_final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```
```{r}
class_final_tree %>%
  extract_fit_engine() %>%
  vip()
```
# Random Forest
```{r}
class_rf_recipe <- 
  recipe(High ~ ., data = Carseats_train) |>
  step_naomit(all_predictors()) |>
  step_dummy(all_nominal(), -all_outcomes()) |>
  step_center(all_predictors(), -all_outcomes()) |>
  step_scale(all_predictors(), -all_outcomes())
```
```{r}
class_rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),
    trees = tune()
  ) |>
  set_engine("ranger")
```
```{r}
class_rf_workflow <- workflow() %>%
  add_recipe(class_rf_recipe) %>%
  add_model(class_rf_mod)
```
```{r}
class_rf_grid <- grid_regular(mtry(range = c(1L, 5L)), trees(range = c(100L, 300L)), levels = c(5, 5))
```
```{r}
set.seed(123)
class_folds <- vfold_cv(Carseats_train, v = 5)
class_rf_fit <- class_rf_workflow %>%
  tune_grid(
    resamples = class_folds,
    grid = class_rf_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
```
```{r}
class_best_rf <- class_rf_fit %>%
  select_best(metric = "roc_auc")
class_rf_final_workflow <- class_rf_workflow %>%
  finalize_workflow(class_best_rf)
class_rf_final_fit <- class_rf_final_workflow %>%
  last_fit(data_split)
class_rf_final_fit %>%
  collect_metrics()
```
# Boosting
```{r}
High <- factor(ifelse(Carseats$Sales > 8, "yes", "no"))
Carseats_class <- data.frame(Carseats, High)
Carseats_class <- Carseats_class %>% select(-Sales)
```
```{r}
class_boost_recipe <- 
  recipe(High ~ ., data = Carseats_train) |>
  step_naomit(all_predictors()) |>
  step_dummy(all_nominal(), -all_outcomes()) |>
  step_center(all_predictors(), -all_outcomes()) |>
  step_scale(all_predictors(), -all_outcomes())
```
```{r}
class_boost_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
```
```{r}
class_boost_workflow <- workflow() %>%
  add_recipe(class_boost_recipe) %>%
  add_model(class_boost_mod)
```
```{r}
class_boost_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
```
```{r}
set.seed(123)
class_folds <- vfold_cv(Carseats_train, v = 5)
class_boost_fit <- class_boost_workflow %>%
  tune_grid(
    resamples = class_folds,
    grid = class_boost_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
```
```{r}
class_best_boost <- class_boost_fit %>%
  select_best(metric = "roc_auc")
class_boost_final_workflow <- class_boost_workflow %>%
  finalize_workflow(class_best_boost)
class_boost_final_fit <- class_boost_final_workflow %>%
  last_fit(data_split)
class_boost_final_fit %>%
  collect_metrics()
```
```{r}
class_boost_final_tree <- extract_workflow(class_boost_final_fit)
class_boost_final_tree %>%
  extract_fit_engine() %>%
  vip()
```

