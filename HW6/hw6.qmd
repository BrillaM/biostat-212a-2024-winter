---
title: "Biostat 212a Homework 6"
subtitle: "Due Mar 22, 2024 @ 11:59PM"
author: "YOUR NAME and UID"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

Load R libraries.
```{r}
library(tidyverse)
library(tidymodels)
library(readr)
library(tswge)
library(ggplot2)
library(ggthemes)
library(recipes)
library(Metrics)

acfdf <- function(vec) {
  vacf <- acf(vec, plot = F)
  with(vacf, data.frame(lag, acf))
}
ggacf <- function(vec) {
  ac <- acfdf(vec)
  ggplot(data = ac, aes(x = lag, y = acf)) +
    geom_hline(aes(yintercept = 0)) +
    geom_segment(mapping = aes(xend = lag, yend = 0))
}

tplot <- function(vec) {
  df <- data.frame(X = vec, t = seq_along(vec))
  ggplot(data = df, aes(x = t, y = X)) +
    geom_line()
}
```

## New York Stock Exchange (NYSE) data (1962-1986) (140 pts)

::: {#fig-nyse}

<p align="center">
![](ISL_fig_10_14.pdf){width=600px height=600px}
</p>

Historical trading statistics from the New York Stock Exchange. Daily values of the normalized log trading volume, DJIA return, and log volatility are shown for a 24-year period from 1962-1986. We wish to predict trading volume on any day, given the history on all earlier days. To the left of the red bar (January 2, 1980) is training data, and to the right test data.

:::

The [`NYSE.csv`](https://raw.githubusercontent.com/ucla-econ-425t/2023winter/master/slides/data/NYSE.csv) file contains three daily time series from the New York Stock Exchange (NYSE) for the period Dec 3, 1962-Dec 31, 1986 (6,051 trading days).

- `Log trading volume` ($v_t$): This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.
    
- `Dow Jones return` ($r_t$): This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.
    
- `Log volatility` ($z_t$): This is based on the absolute values of daily price movements.

```{r}
# Read in NYSE data from url

url <- "https://raw.githubusercontent.com/ucla-biostat-212a/2024winter/master/slides/data/NYSE.csv"
NYSE <- read_csv(url)

NYSE
```
The **autocorrelation** at lag $\ell$ is the correlation of all pairs $(v_t, v_{t-\ell})$ that are $\ell$ trading days apart. These sizable correlations give us confidence that past values will be helpful in predicting the future.

```{r}
#| code-fold: true
#| label: fig-nyse-autocor
#| fig-cap: "The autocorrelation function for log volume. We see that nearby values are fairly strongly correlated, with correlations above 0.2 as far as 20 days apart."

ggacf(NYSE$log_volume) + ggthemes::theme_few()
```

Do a similar plot for (1) the correlation between $v_t$ and lag $\ell$ `Dow Jones return` $r_{t-\ell}$ and (2) correlation between $v_t$ and lag $\ell$ `Log volatility` $z_{t-\ell}$.

```{r}
seq(1, 30) %>%
  map(function(x) {
    cor(NYSE$log_volume, lag(NYSE$DJ_return, x), use = "pairwise.complete.obs")
  }) %>%
  unlist() %>%
  tibble(lag = 1:30, cor = .) %>%
  ggplot(aes(x = lag, y = cor)) +
  geom_hline(aes(yintercept = 0)) +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  ggtitle("AutoCorrelation between `log volume` and lagged `DJ return`")
```
```{r}
seq(1, 30) %>%
  map(function(x) {
    cor(NYSE$log_volume, lag(NYSE$log_volatility, x), use = "pairwise.complete.obs")
  }) %>%
  unlist() %>%
  tibble(lag = 1:30, cor = .) %>%
  ggplot(aes(x = lag, y = cor)) +
  geom_hline(aes(yintercept = 0)) +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  ggtitle("AutoCorrelation between `log volume` and lagged `log volatility`")
```
### Project goal

Our goal is to forecast daily `Log trading volume`, using various machine learning algorithms we learnt in this class. 

The data set is already split into train (before Jan 1st, 1980, $n_{\text{train}} = 4,281$) and test (after Jan 1st, 1980, $n_{\text{test}} = 1,770$) sets.

<!-- Include `day_of_week` as a predictor in the models. -->

In general, we will tune the lag $L$ to acheive best forecasting performance. In this project, we would fix $L=5$. That is we always use the previous five trading days' data to forecast today's `log trading volume`.

Pay attention to the nuance of splitting time series data for cross validation. Study and use the [`time-series`](https://www.tidymodels.org/learn/models/time-series/) functionality in tidymodels. Make sure to use the same splits when tuning different machine learning algorithms.

Use the $R^2$ between forecast and actual values as the cross validation and test evaluation criterion.

```{r}
#| code-fold: true
#| label: fig-nyse-train-test
#| fig-cap: "The training and test sets for the NYSE data. The training set consists of the first 4,281 trading days, and the test set consists of the remaining 1,770 trading days."
```

### Baseline method (20 pts)

We use the straw man (use yesterdayâ€™s value of `log trading volume` to predict that of today) as the baseline method. Evaluate the $R^2$ of this method on the test data.

```{r}
# Lag: look back L trading days
# Do not need to include, as we included them in receipe
L <- 5

for (i in seq(1, L)) {
  NYSE <- NYSE %>%
    mutate(
      !!paste("DJ_return_lag", i, sep = "") := lag(NYSE$DJ_return, i),
      !!paste("log_volume_lag", i, sep = "") := lag(NYSE$log_volume, i),
      !!paste("log_volatility_lag", i, sep = "") := lag(NYSE$log_volatility, i)
    )
}


NYSE <- NYSE %>% na.omit()
```

```{r}
# Drop beginning trading days which lack some lagged variables
NYSE_other <- NYSE %>%
  filter(train == "TRUE") %>%
  select(-train) %>%
  drop_na()
dim(NYSE_other)
```

```{r}
NYSE_test <- NYSE %>%
  filter(train == "FALSE") %>%
  select(-train) %>%
  drop_na()
dim(NYSE_test)
```

```{r}
library(yardstick)
# cor(NYSE_test$log_volume, NYSE_test$log_volume_lag1) %>% round(2)
r2_test_strawman <- rsq_vec(NYSE_test$log_volume, lag(NYSE_test$log_volume, 1)) %>% round(2)
print(paste("Straw man test R2: ", r2_test_strawman))
```

**answer**: The $R^2 is 0.35

### Autoregression (AR) forecaster (30 pts)

- Let
$$
y = \begin{pmatrix} v_{L+1} \\ v_{L+2} \\ v_{L+3} \\ \vdots \\ v_T \end{pmatrix}, \quad M = \begin{pmatrix}
1 & v_L & v_{L-1} & \cdots & v_1 \\
1 & v_{L+1} & v_{L} & \cdots & v_2 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & v_{T-1} & v_{T-2} & \cdots & v_{T-L}
\end{pmatrix}.
$$

- Fit an ordinary least squares (OLS) regression of $y$ on $M$, giving
$$
\hat v_t = \hat \beta_0 + \hat \beta_1 v_{t-1} + \hat \beta_2 v_{t-2} + \cdots + \hat \beta_L v_{t-L},
$$
known as an **order-$L$ autoregression** model or **AR($L$)**.

```{r}
NYSE %>%
  ggplot(aes(x = date, y = log_volume)) +
  geom_line() +
  geom_smooth(method = "lm")
```

```{r}
wrong_split <- initial_split(NYSE_other)

bind_rows(
  training(wrong_split) %>% mutate(type = "train"),
  testing(wrong_split) %>% mutate(type = "test")
) %>%
  ggplot(aes(x = date, y = log_volume, color = type, group = NA)) +
  geom_line()
```

```{r}
correct_split <- initial_time_split(NYSE_other %>% arrange(date))

bind_rows(
  training(correct_split) %>% mutate(type = "train"),
  testing(correct_split) %>% mutate(type = "test")
) %>%
  ggplot(aes(x = date, y = log_volume, color = type, group = NA)) +
  geom_line()
```

```{r}
rolling_origin(NYSE_other %>% arrange(date), initial = 30, assess = 7) %>%
  # sliding_period(NYSE_other %>% arrange(date), date, period = "day", lookback = Inf, assess_stop = 1) %>%
  mutate(
    train_data = map(splits, analysis),
    test_data = map(splits, assessment)
  ) %>%
  select(-splits) %>%
  pivot_longer(-id) %>%
  filter(id %in% c("Slice0001", "Slice0002", "Slice0003")) %>%
  unnest(value) %>%
  ggplot(aes(x = date, y = log_volume, color = name, group = NA)) +
  geom_point() +
  geom_line() +
  facet_wrap(~id, scales = "fixed")
```

```{r}
sliding_period(NYSE_other %>% arrange(date),
  date,
  period = "month", lookback = Inf, assess_stop = 1
) %>%
  mutate(
    train_data = map(splits, analysis),
    test_data = map(splits, assessment)
  ) %>%
  select(-splits) %>%
  pivot_longer(-id) %>%
  filter(id %in% c("Slice001", "Slice002", "Slice003")) %>%
  unnest(value) %>%
  ggplot(aes(x = date, y = log_volume, color = name, group = NA)) +
  geom_point() +
  geom_line() +
  facet_wrap(~id, scales = "fixed")
```
- Tune AR(5) with elastic net (lasso + ridge) regularization using all 3 features on the training data, and evaluate the test performance. 

```{r}
en_receipe <- 
  recipe(log_volume ~ DJ_return_lag1 + DJ_return_lag2 + DJ_return_lag3 +
           DJ_return_lag4 + DJ_return_lag5 + log_volume_lag1+
           log_volume_lag2 +log_volume_lag3 + log_volume_lag4 +
           log_volume_lag5 + log_volatility_lag1 + log_volatility_lag2 +
           log_volatility_lag3 + log_volatility_lag4 + log_volatility_lag5, data = NYSE_other) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_numeric_predictors(), -all_outcomes()) %>%  
  step_naomit(all_predictors()) #%>%
  #prep(data = NYSE_other)
```

```{r}
### Model
enet_mod <- 
  # mixture = 0 (ridge), mixture = 1 (lasso)
  # mixture = (0, 1) elastic net 
  # As an example, we set mixture = 0.5. It needs to be tuned.
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")
enet_mod
```

```{r}
en_wf <- 
  workflow() %>%
  add_model(enet_mod) %>%
  add_recipe(en_receipe %>% step_indicate_na())
en_wf
```

```{r}
folds <- NYSE_other %>%
  arrange(date) %>%
  sliding_period(date, period = "month", lookback = Inf, assess_stop = 1)
# rolling_origin(initial = 5, assess = 1)


month_folds <- NYSE_other %>%
  sliding_period(
    date,
    "month",
    lookback = Inf,
    skip = 4
  )
```

```{r}
lambda_grid <-
  grid_regular(penalty(range = c(-8, -7), trans = log10_trans()), 
               mixture(range = c(0,1)), levels = 3)
lambda_grid
```

```{r}
en_fit <- tune_grid(en_wf, resamples = month_folds, grid = lambda_grid) 
en_fit
```
Visualize CV criterion
```{r}
en_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rsq") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Penalty", y = "CV Rsq") + 
  scale_x_log10(labels = scales::label_number())
```

```{r}
en_fit %>%
  show_best("rsq")
```

```{r}
results <- en_fit %>%
  collect_metrics()

best_lasso <- results %>%
  filter(.metric == "rsq") %>%
  slice_min(mean, with_ties = FALSE)
best_lasso
```

```{r}
# Final workflow
final_wf <- en_wf %>%
  finalize_workflow(best_lasso)
final_wf
```

```{r}
final_fit <- 
  final_wf %>%
  fit(data = NYSE_other)
predictions <- predict(final_fit, new_data = NYSE_test)
predictions
```

```{r}
results <- bind_cols(NYSE_test, predictions)
en_rmse_val <- yardstick::rmse(results, truth = log_volume, estimate = .pred)
en_rsq_val <- yardstick::rsq(results, truth = log_volume, estimate = .pred)

print(en_rmse_val)
print(en_rsq_val)
```

- Hint: [Workflow: Lasso](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) is a good starting point.

### Random forest forecaster (30pts)

- Use the same features as in AR($L$) for the random forest. Tune the random forest and evaluate the test performance.


```{r}
rf_mod <-
  rand_forest(
    mode = "regression",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>%
  set_engine("ranger")
rf_mod
```

```{r}
rf_wf <- workflow() %>%
  add_recipe(en_receipe %>% step_indicate_na()) %>%
  add_model(rf_mod)
rf_wf
```

```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)),
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
)
param_grid
```

```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = month_folds,
    grid = param_grid,
    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)
  )
rf_fit
```

```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() +
  geom_line() +
  labs(x = "Num. of Trees", y = "CV mse")
```

```{r}
rf_fit %>%
  show_best("rsq")
```

```{r}
best_rf <- rf_fit %>%
  select_best("rsq")
best_rf
```

```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  fit(data = NYSE_other)
predictions <- predict(final_fit, new_data = NYSE_test)
predictions
```

```{r}
results <- bind_cols(NYSE_test, predictions)
en_rmse_val <- yardstick::rmse(results, truth = log_volume, estimate = .pred)
en_rsq_val <- yardstick::rsq(results, truth = log_volume, estimate = .pred)

print(en_rmse_val)
print(en_rsq_val)
```


- Hint: [Workflow: Random Forest for Prediction](https://ucla-biostat-212a.github.io/2024winter/slides/08-tree/workflow_rf_reg.html) is a good starting point.

### Boosting forecaster (30pts)

- Use the same features as in AR($L$) for the boosting. Tune the boosting algorithm and evaluate the test performance.

- Hint: [Workflow: Boosting tree for Prediction](https://ucla-biostat-212a.github.io/2024winter/slides/08-tree/workflow_boosting_reg.html) is a good starting point.


```{r}
gb_mod <-
  boost_tree(
    mode = "regression",
    trees = 1000,
    tree_depth = tune(),
    learn_rate = tune()
  ) %>%
  set_engine("xgboost")
```

```{r}
gb_wf <- workflow() %>%
  add_recipe(en_receipe) %>%
  add_model(gb_mod)
gb_wf
```

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 4L)),
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(4, 10)
)
param_grid
```



```{r}
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = month_folds,
    grid = param_grid,
    metrics = yardstick::metric_set(yardstick::rmse, yardstick::rsq)
  )
gb_fit
```

```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rsq") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```
```{r}
gb_fit %>%
  show_best("rsq")
```

```{r}
best_gb <- gb_fit %>%
  select_best("rsq")
best_gb
```

```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf
```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  fit(data = NYSE_other)
predictions <- predict(final_fit, new_data = NYSE_test)
predictions
```

```{r}
results <- bind_cols(NYSE_test, predictions)
en_rmse_val <- yardstick::rmse(results, truth = log_volume, estimate = .pred)
en_rsq_val <- yardstick::rsq(results, truth = log_volume, estimate = .pred)

print(en_rmse_val)
print(en_rsq_val)
```

### Summary (30pts)

Your score for this question is largely determined by your final test performance.

Summarize the performance of different machine learning forecasters in the following format. 

| Method | CV $R^2$ | Test $R^2$ |
|:------:|:------:|:------:|:------:|
| Baseline |-- |0.35 | |
| AR(5) |0.2503034 | 0.402755| |
| Random Forest |0.2501541 |0.4008558	 | |
| Boosting |0.2587130 |0.4013643 | |

## ISL Exercise 12.6.13 (90 pts)
 ### 12.6.13 (b) (30 pts)

```{r}
Ch12Ex13 <- read.csv("Ch12Ex13.csv", header = F)

```
```{r}
dd <- as.dist(1 - cor(Ch12Ex13))

hclust_ex11 <- function(method_hclust) {
  corr_clust <- hclust(dd, method = method_hclust)

  factoextra::fviz_dend(
    corr_clust,
    k = 2,
    #
    cex = 0.5,
    # label size
    k_colors = c("limegreen", "skyblue"),
    color_labels_by_k = TRUE # Change theme
  )
}
hclust_ex11("complete")
```



### PCA and UMAP (30 pts)

```{r}
library(recipes)
library(ggplot2)
library(embed)
library(tidymodels)
library(ISLR2)
```


```{r}
Ch12Ex13_2 <- Ch12Ex13 %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column("ID") %>%
  mutate(
    numericID = as.numeric(sub("^[^0-9]+", "", ID)), # Replace non-numeric prefixes with ""
    status = ifelse(numericID <= 20, "healthy", "diseased")
  ) %>%
  select(-numericID) # Optionally remove the numericID column if not needed


```

###PCA

```{r}
pca_rec <- recipe(~., data = Ch12Ex13_2) %>%
  update_role(ID,status, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())
pca_prep <- prep(pca_rec)
pca_prep
```

```{r}
library(tidytext)
tidied_pca <- tidy(pca_prep, 2)


tidied_pca %>%
  filter(component %in% paste0("PC", 1:4)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  )  
```
```{r}
juice(pca_prep) %>%
  ggplot(aes(PC1, PC2, label = ID)) +
  geom_point(aes(color = status), alpha = 0.7, size = 2) +
  # geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL)
```
###UMAP

```{r}
library(embed)
umap_rec<- recipe(~., data = Ch12Ex13_2) %>%
  update_role(ID,status, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_umap(all_predictors())
umap_prep <- prep(umap_rec)
umap_prep
```
```{r}
juice(umap_prep) %>%
  ggplot(aes(UMAP1, UMAP2, label = ID)) +
  geom_point(aes(color = status), alpha = 0.7, size = 2) +
  #geom_text(check_overlap = TRUE, hjust = "inward") +
  labs(color = NULL)
```

### 12.6.13 (c) (30 pts)

```{r}
library(tidyclust)
Ch12Ex13_1 <- read_csv("Ch12Ex13.csv", col_names = paste("ID", 1:40, sep = ""))
```

```{r}
set.seed(838383)
hc_spec <- hier_clust(
  # num_clusters = 3,
  linkage_method = "average"
)


hc_fit <- hc_spec %>%
  fit(~ .,
    data = as.data.frame(t(Ch12Ex13_1)) 
  )

hc_fit %>%
  summary()
```

```{r}
hc_fit$fit %>% plot()
```

```{r}
grp = factor(rep(c(1, 0), each = 20))

regression <- function(y) {
  sum <- summary(lm(y ~ grp))
  pv <- sum$coefficients[2, 4]
  return(pv)
}


out <- tibble(gene = seq(1, nrow(Ch12Ex13_1)),
              p_values = unlist(purrr:: map(1:nrow(Ch12Ex13_1), ~regression(as.matrix(Ch12Ex13_1)[.x, ]))))
```

```{r}
out %>% arrange(p_values) %>% head(10)
```

```{r}
# sig <- out %>% arrange(p_values) %>% filter(p_values < 0.05/nrow(Ch12Ex13))
sig <- out %>% arrange(p_values) %>% filter(p_values < 0.05 )
```

```{r}
# install.packages("pheatmap")
library(pheatmap)
# install.packages("ggplotify")
library(ggplotify) ## to convert pheatmap to ggplot2
# install.packages("heatmaply")
library(heatmaply) ## for constructing interactive heatmap
```

```{r}
#create data frame for annotations
dfh <- data.frame(sample=as.character(colnames(Ch12Ex13_1)), status = "disease") %>%
                column_to_rownames("sample")
dfh$status[seq(21, 40)] <-  "healthy"
dfh
```

```{r}
pheatmap(Ch12Ex13_1[sig$gene, ], cluster_rows = FALSE, cluster_cols = T, scale="row", annotation_col = dfh,
         annotation_colors=list(status = c(disease = "orange", healthy = "black")),
         color=colorRampPalette(c("navy", "white", "red"))(50))
```