   ---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "Brilla Meng UID: 806329681"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)
a.
**answer:** The probability that the /th observation is in the test set is $1/n$. The probability that it is not in the test set is $1-1/n$. 

b.
**answer:** The probability that the /th observation is the second bootstrap sample. The probability that the /th is not the second bootstrap sample is 1-1/n.

c.
**answer:** As we use sampling with replacement to generate the bootstrap sample, the selection probability are independent, the probability(Observation J is not the first bootstrap observation, not the second, not the nth)= Probablity (Observation J is not the first bootstrap observation) * Probablity (Observation J is not the second bootstrap observation) * ... * Probablity (Observation J is not the nth bootstrap observation) = (1-1/n)^n

d.
**answer:** for n=5, 1-(1-1/n)^n = 1-(1-1/5)^5 = 0.67232

e.
**answer:** for n=100, 1-(1-1/n)^n = 1-(1-1/100)^100 = 0.63397

f.
**answer:** for n=10000, 1-(1-1/n)^n = 1-(1-1/10000)^10000 = 0.63214

g.

```{r}
n <- 1:100000
y <- 1 - (1 - 1/n)^n
plot(n, y, type = "l", log = "x", xlab = "n", ylab = "1 - (1 - 1/n)^n")
abline(h = 1 - exp(-1), lty = "dotted")
```

**answer:**
The limit of (1-1/n)^n as n goes to infiinity is exp(-1), as n gets larger, the limit of (1-1/n)^n is getting closer to exp(-1), which is approximately 0.6321.

h.

```{r}
store <- rep(NA, 10000) 
for(i in 1:10000){
  store[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0 
  }
mean(store)
```

**answer:** The probability that the 4th observation is in the bootstrap sample is 0.6396, which is close to the limit of (1-1/n)^n as n goes to infiinity, which is exp(-1), approximately 0.6321.

## ISL Exercise 5.4.9 (20pts)

```{r}
library(MASS)
library(ISLR2)
library(boot)
```

a.

```{r}
mu.hat <- mean(Boston$medv)
mu.hat
```

**answer:** The estimate μˆ is 22.53281.

b.

```{r}
SE = sd(Boston$medv)/sqrt(nrow(Boston))
SE
```

**answer:** The standard error  is 0.4088611. This is very small, we can interpret that the sample mean is very close to the true population mean.

c.

```{r}
set.seed(1)
boot.fn = function(data, index) {
  mu <- mean(data[index])
  return (mu)
}
SE_boot = boot(Boston$medv, boot.fn, R=1000)
SE_boot
```

**answer:** The bootstrap estimate for the standard error of μˆ is 0.4106622,
where the standard error of the mean is 0.4088611.  The bootstrap estimate is very close to the standard error of the mean.

d.

```{r}
mu.hat_boot = SE_boot$t0
SE_boot = sd(SE_boot$t)
CI <- c(mu.hat_boot - 2*SE_boot, mu.hat_boot + 2*SE_boot)
CI
```
```{r}
t.test(Boston$medv)$conf.int
```

**answer:** The 95% confidence interval for μˆ is (21.72953, 23.33608), where the 95% confidence interval for the mean is (21.71148, 23.35413). The bootstrap estimate is very close to the t.test estimate.

e.

```{r}
median.hat <- median(Boston$medv)
median.hat
```

**answer:** The estimate for the median is 21.2.

f.

```{r}
set.seed(1)
boot.fn = function(data, index) {
  median <- median(data[index])
  return (median)
}
SE_boot_median= boot(Boston$medv, boot.fn, R=1000)
SE_boot_median
```

**answer:** The bootstrap estimate for the standard error of the median is 0.3778075.

g.

```{r}
mu.hat.0.1 = quantile(Boston$medv, 0.1)
mu.hat.0.1
```

**answer:** The estimate for the 10th percentile is 12.75.

h.

```{r}
set.seed(1)
boot.fn = function(data, index) {
  mu.0.1 <- quantile(data[index], 0.1)
  return (mu.0.1)
}
se_boot_quant = boot(Boston$medv, boot.fn, R=1000)
se_boot_quant
```

**answer:** The bootstrap estimate for the standard error of the 10th percentile is 0.4767526.

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

$$
C_p = \frac{1}{n} ( RSS + 2d\hat{\sigma}^2 )
$$

$$
AIC = -2\log L + 2d
$$
Given the linear model with Gasussian errors which  $\epsilon \sim N(0, \sigma^2I)$, the likelihood function is
$$
L = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp(-\frac{1}{2\sigma^2} (\epsilon_i)^2 ) = (\frac{1}{(2\pi\sigma^2)^{n/2}}) \exp(-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_{i})^2)
$$

Maximizing the likelihood is equivalent to minimizing $\exp(-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (\epsilon)^2)$ which is the least squares estimate.

$$
\frac{\partial}{\partial \beta_0} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_{i})^2 = 0, \quad \frac{\partial}{\partial \beta_1} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_{i})^2 = 0
$$
The solution is same as the lease squares estimate, which means MLE of $\beta_0+\beta_1$ is same as the LSE of $\beta_0+\beta_1$.
We know that $C_p = \frac{1}{n} ( RSS + 2d\hat{\sigma}^2)$ and $AIC = -2\log L + 2d$ where L is the maximized likelihood. 
$$
\begin{align*}
AIC = -2\log(L) + 2d \\
= -2(-\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2) + 2d \\
= n\log(2\pi\sigma^2) + \frac{1}{\sigma^2} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + 2d \\
= n\log(2\pi\sigma^2) + \frac{RSS}{\sigma^2} + 2d \\
= n\log(2\pi) + n\log(\sigma^2) + \frac{RSS}{\sigma^2} + 2d \\
= n\log(2\pi) + n\log(\frac{RSS}{n}) + n + 2d \\
= n\log(2\pi \sigma^2) + \frac n {\sigma^2}C_p \\
\end{align*}
$$
Since they differ by a constant which is only a function of constants $\sigma^2$ and n, it doesn't affect the model selection. Therefore, $C_p$ and AIC are equivalent.


## ISL Exercise 6.6.1 (10pts)

a.

**answer:** Best subset selection has the smallest training RSS. Both forward and backward selection determine models that depend on which predictors they pick first as they repeat toward the k^th predictor. It means choose a wrong choice early can't be undone.

b.

**answer:** Best subset selection may have the smallest test RSS because it takes into account all possible models, but it may also have the largest test RSS because it may overfit the training data. Forward and backward selection may have a larger test RSS than best subset selection because they may not take into account all possible models, but they may also have a smaller test RSS than best subset selection because they may not overfit the training data.


c.

i.
**answer:** True, the model with (k+1) predictors is obtained by augmenting the predictors in the model with k predictors that have the addtional predictor.

ii.
**answer:** True, the model with k predictors is obtained by removing one predictor from the model with (k+1) predictors.

iii.
**answer:** False. There is no direct link between the models obtained from forward and backward selection.

iv.
**answer:** False. There is no direct link between the models obtained from forward and backward selection.

v.
**answer:** False. The model with(k+1) predictors is obtained by selecting among all possible models with (k+1) predictors, does not necessarily contain all the predictors slected for the k-variable model.

## ISL Exercise 6.6.3 (10pts)

a.
**answer:** Steadily decrease, as increase s from 0, we are restricting the βj coefficients less and less. The model is becoming more flexible, and the training RSS will decrease.

b.
**answer:** Decrease initially, and then eventually start increasing in a U shape. As we increase s from 0, we are restricting the βj coefficients less and less. The model is becoming more flexible, and the test RSS will decrease. However, after a certain point, the model will become too flexible and will overfit the training data, so the test RSS will begin to increase.

c.
**answer:** Steadily increase. As we increase s from 0, we are restricting the βj coefficients less and less. so the model is becoming more and more flexible which provokes a steady increase in variance.

d.
**answer:** Steadily decrease. As we increase s from 0, we are restricting the βj coefficients less and less. The model is becoming more flexible, and the bias will decrease.

e.
**answer:** Remain constant.By definition, the variance of the error term is constant, so the variance of the residuals will remain constant.



## ISL Exercise 6.6.4 (10pts)

a.
**answer:** Steadily increase. As we increase s from 0, we are restricting the βj coefficients less and less. The model is becoming more flexible, and the variance of the residuals will increase.

b.
**answer:** Decrease initially, and then eventually start increasing in a U shape. As we increase s from 0, we are restricting the βj coefficients less and less. The model is becoming more flexible, and the test RSS will decrease. However, after a certain point, the model will become too flexible and will overfit the training data, so the test RSS will begin to increase.

c.
**answer:** Steadily decrease. As we increase s from 0, we are restricting the βj coefficients more and more. The model is becoming more less and less flexible which provokes a steady increase in variance.

d.
**answer:**Steadily increase. As we increase s from 0, we are restricting the βj coefficients more and more. The model is becoming more less and less flexible which provokes a steady increase in bias

e.
**answer:**Remain constant.By definition, the irreducible of the error term is independent of the model, and consequently independant of the value of λ.

## ISL Exercise 6.6.5 (10pts)

a.
**answer:** With the infomation from $x_{11}= x_{12} = x_1$ and $x_{21}= x_{22} = x_2$,the ridge regression problem can be written as
$$
(y_1 - \hat{\beta}_1 x_1 - \hat{\beta}_2 x_1)^2 + (y_2 - \hat{\beta}_1 x_2 - \hat{\beta}_2 x_2)^2 + \lambda(\hat{\beta}_1^2 + \hat{\beta}_2^2)
$$

b.
**answer:** Argue that in this setting, the ridge coefficient estimates satisfy $\hat{\beta}_1 = \hat{\beta}_2$.By taking the derivative of the ridge regression problem with respect to $\hat{\beta}_1$ and $\hat{\beta}_2$ and setting them equal to 0, we can get
$$
\hat{\beta}_1 (x_1^2 + x_2^2 + \lambda)+\hat{\beta}_2(x_1^2 + x_2^2) = x_1 y_1 + x_2 y_2 \\
$$
and
$$
\hat{\beta}_1(x_1^2 + x_2^2) + \hat{\beta}_2(x_1^2 + x_2^2 + \lambda) = x_1 y_1 + x_2 y_2 
$$
$$
\lambda\hat{\beta}_1 =
x_1y_1+x_2y_2+2\hat{\beta}_1x_1x_2+2\hat{\beta}_2x_1x_2 
\\ \lambda\hat{\beta}_2 =
x_1y_1+x_2y_2+2\hat{\beta}_1x_1x_2+2\hat{\beta}_2x_1x_2
$$
we can get $\hat{\beta}_1 = \hat{\beta}_2$

c.
**answer:** According to this setting $x_{11}= x_{12} = x_1$ and $x_{21}= x_{22} = x_2$, the lasso optimization problem can be written as
$$
(y_1-\hat{\beta}_1x_1-\hat{\beta}_2x_1)^2+(y_2-\hat{\beta}_1x_2-\hat{\beta}_2x_2)^2+\lambda(|\hat{\beta}_1|+|\hat{\beta}_2|)
$$

d.
**answer:** 

$$
(y_1-\hat{\beta}_1x_1-\hat{\beta}_2x_1)^2+(y_2-\hat{\beta}_1x_2-\hat{\beta}_2x_2)^2 
$$

subject to 

$$
|\hat{\beta}_1|+|\hat{\beta}_2| \leq s
$$

The lasso constraint is a diamond shape, and the ridge constraint is a circle and centered at the origin of the plane$\hat{\beta}_1,\hat{\beta}_2$ which has distance s from the origin. By using the setting of this question,$x_{11}= x_{12} = x_1$ and $x_{21}= x_{22} = x_2$, $x_1+x_2 =0$ and $y_1+y_2=0$. we can get 

$$
2[y_1-(\hat{\beta}_1+\hat{\beta}_2)x_1]^2 \geq 0
$$

The line parallel to the edge of the diamond of the constraints. Now, solutions to the lasso optimization problem are contours of the function $ y_1-(\hat{\beta}_1+\hat{beta}_2)x_1)^2 $ that intersects the diamond of the constraints. The entire edge $\hat{\beta}_1+\hat{\beta}_2=s$ is a potential solution. The lasso optimization problem has a solutions:

$$
\{ (\hat{\beta}_1, \hat{\beta}_2) : \hat{\beta}_1 + \hat{\beta}_2 = s \text{ with } \hat{\beta}_1, \hat{\beta}_2 \geq 0 \text{ and } \hat{\beta}_1 + \hat{\beta}_2 = -s \text{ with } \hat{\beta}_1, \hat{\beta}_2 \leq 0 \}.
$$

## ISL Exercise 6.6.11 (30pts)


```{r}
library(glmnet)
library(MASS)
library(ISLR2)
library(tidyverse)
library(tidymodels)
library(GGally)
data("Boston")
head(Boston)
```
```{r}
Boston <- Boston %>%
  drop_na()
dim(Boston)
```
```{r}
set.seed(425)
data_split = initial_split(
  Boston,
  prop = 0.75,
  strata = "crim"
)

Boston_other <- training(data_split)
dim(Boston_other)
Boston_test <- testing(data_split)
dim(Boston_test)
```
```{r}
norm_recipe <- 
  recipe(
    crim ~ ., 
    data = Boston_other
  ) %>%
  # create traditional dummy variables
  step_dummy(all_nominal()) %>%
  # zero-variance filter
  step_zv(all_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_predictors())
```
```{r}
ls_mod <- 
  linear_reg() %>% 
  set_engine("lm")

lasso_mod <- 
  # mixture = 0 (ridge), mixture = 1 (lasso)
  linear_reg(penalty = tune(), mixture = 1.0) %>% 
  set_engine("glmnet")

ridge_mod <- 
  linear_reg(penalty = tune(), mixture = 0.0) %>% 
  set_engine("glmnet")
```
```{r}
ls_workflow <- 
  workflow() %>%
  add_recipe(norm_recipe) %>%
  add_model(ls_mod) 

lasso_workflow <- 
  workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(norm_recipe)

ridge_workflow <- 
  workflow() %>%
  add_model(ridge_mod) %>%
  add_recipe(norm_recipe)

lambda_grid <-
  grid_regular(penalty(range = c(-2, 3), trans = log10_trans()), levels = 100)
lambda_grid
```
```{r}
folds <- vfold_cv(Boston_other, v = 10)
folds
```
```{r}
ls_fit <- 
  ls_workflow %>%
  fit(data = Boston_other)
predicted_values <- 
  predict(ls_fit, Boston_other) %>% pull(.pred)

ls_cv_rmse <- 
  sqrt(mean((Boston_other$crim - predicted_values)^2))

lasso_fit<-
  lasso_workflow %>%
  tune_grid(resamples = folds, grid = lambda_grid)

ridge_fit<-
  ridge_workflow %>%
  tune_grid(resamples = folds, grid = lambda_grid)

lasso_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Penalty", y = "CV RMSE") + 
  scale_x_log10(labels = scales::label_number())
```
```{r}
lassotibble <- lasso_fit %>%
  show_best("rmse")

lasso_cv_rmse <- lassotibble[[1,4]]

best_lasso <- lasso_fit %>%
  select_best("rmse")
best_lasso
```
```{r}
ridge_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  labs(x = "Penalty", y = "CV RMSE") + 
  scale_x_log10(labels = scales::label_number())
```
```{r}
ridgetibble <- ridge_fit %>%
  show_best("rmse")

ridge_cv_rmse <- ridgetibble[[1,4]]

best_ridge <- ridge_fit %>%
  select_best("rmse")
best_ridge
```
 
b.

```{r}
final_lasso <- lasso_workflow %>%
  finalize_workflow(best_lasso)

final_ridge <- ridge_workflow %>%
  finalize_workflow(best_ridge)

final_lasso_fit <- 
  final_lasso %>%
  last_fit(data_split)

final_ridge_fit <-
  final_ridge %>%
  last_fit(data_split)

final_ls_fit <-
  ls_workflow %>%
  last_fit(data_split)

final_lasso_tibble <- 
  final_lasso_fit %>%
  collect_metrics() 
  
final_ridge_tibble <- 
  final_ridge_fit %>%
  collect_metrics() 

final_ls_tibble <- 
  final_ls_fit %>%
  collect_metrics() 

ls_test_rmse <- 
  final_ls_tibble [[1,3]]
lasso_test_rmse <-
  final_lasso_tibble [[1,3]]
ridge_test_rmse <-
  final_ridge_tibble [[1,3]]
```


You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

```{r}
rmse_results <- data.frame(
  Method = c("LS", "Ridge", "Lasso"),
  `CV RMSE` = c(ls_cv_rmse, ridge_cv_rmse, lasso_cv_rmse),
  `Test RMSE` = c(ls_test_rmse, ridge_test_rmse, lasso_test_rmse)
)
rmse_results
```
| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS |5.590479 | 8.375181| |
| Ridge |4.957944 |8.448881 | |
| Lasso |4.966202 |8.441008 | |
| ... | | | |

**answer**: The Lasso model has the cross-validation RMSE of 4.966202. The Ridge model has a best cross-
validation RMSE of 4.957944 The least squares model has a cross-validation RMSE of 5.590479. The Ridge model has the lowest cross-validation RMSE, so it is the best
model.

c.
**answer**: Lasso model doesn't involve all futures in the data set. It uses a penalty to shrink the coefficients of some features to zero. It does not involve any penalty.

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$
**answer**: To show that $\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].$, we can use the following steps:
$$
E[R_{\text{train}}(\hat{\beta})] = \frac{1}{N} \sum_{i=1}^N E[(y_i - \beta^T x_i)^2] = \frac{1}{N} \sum_{i=1}^N E[(y_i - E[y_i] + E[y_i] - \beta^T x_i)^2]
$$
also we have 
$$
E[R_{\text{test}}(\hat{\beta})] = \frac{1}{M} \sum_{i=1}^M E[(y_i - \beta^T x_i)^2] = \frac{1}{M} \sum_{i=1}^M E[(y_i - E[y_i] + E[y_i] - \beta^T x_i)^2]
$$
which means that $E[R_{\text{train}}(\hat{\beta})]$ is equal to $E[R_{\text{test}}(\hat{\beta})]$ and we know that $E[R_{\text{train}}(\hat{\beta})] is the least squares estimate which means that $E[R_{\text{train}}(\hat{\beta})] < E[R_{\text{test}}(\hat{\beta})]$. Therefore, we can conclude that $\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].$
Therefore, we have shown that $\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].$ Proof is complete. 